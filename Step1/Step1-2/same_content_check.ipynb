{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from konlpy.tag import Okt\n",
    "import re\n",
    "import os\n",
    "import networkx as nx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Load the TSV file\n",
    "tsv_file_path = \"C:\\\\Users\\\\WINDOWS11\\\\Desktop\\\\kpop_agenda\\\\Step1\\\\articles_metadata.tsv\"  # Use raw string or escape backslashes\n",
    "df = pd.read_csv(tsv_file_path, sep='\\t', encoding='utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Load articles and preprocess with KoNLPy for NER (Person entities) and basic cleaning\n",
    "articles_ner_info = {} # Store NER info for each article, including original text\n",
    "article_ids = []    # Store corresponding IDs\n",
    "okt = Okt()\n",
    "\n",
    "def extract_ner_information(article_content):\n",
    "    \"\"\"\n",
    "    Extracts Named Entities (specifically PERSONs) using KoNLPy Okt and basic rule-based approach.\n",
    "    Returns PERSON entities and cleaned article text (nouns only).\n",
    "    Relationship keyword extraction is removed.\n",
    "    \"\"\"\n",
    "    pos_tags = okt.pos(article_content, norm=True, stem=True)\n",
    "\n",
    "    person_entities = set()\n",
    "    cleaned_tokens = []\n",
    "\n",
    "    for token, tag in pos_tags:\n",
    "        if tag == \"Noun\":\n",
    "            cleaned_tokens.append(token)\n",
    "            # Heuristic for Person names\n",
    "            if tag == \"Noun\" and len(token) > 1 : # names are nouns (usually) and longer than 1 character (limits)\n",
    "                person_entities.add(token) # Basic person entity extraction - can be refined\n",
    "\n",
    "\n",
    "    cleaned_article_text = \" \".join(cleaned_tokens) # Just nouns for embedding (you can adjust this)\n",
    "    return {\n",
    "        \"original_text\": article_content, # Keep original for later if needed\n",
    "        \"person_entities\": person_entities,\n",
    "        \"cleaned_article_text\": cleaned_article_text # Noun tokenized text\n",
    "    }\n",
    "\n",
    "for index, row in df.iterrows():\n",
    "    file_path = row['file_path']\n",
    "    article_id = row['ID']\n",
    "    full_file_path = os.path.join(os.path.dirname(tsv_file_path), file_path) # Construct full file path\n",
    "    try:\n",
    "        with open(full_file_path, 'r', encoding='utf-8') as f:\n",
    "            article_content = f.read()\n",
    "            ner_info = extract_ner_information(article_content) # Extract NER info and cleaned text\n",
    "            articles_ner_info[article_id] = ner_info\n",
    "            article_ids.append(article_id)\n",
    "    except FileNotFoundError:\n",
    "        print(f\"Warning: File not found: {full_file_path}\")\n",
    "        articles_ner_info[article_id] = {\"person_entities\": set(), \"cleaned_article_text\": \"\"} # Empty info for missing files\n",
    "        article_ids.append(article_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Sentence Embeddings (using cleaned noun-based text)\n",
    "model = SentenceTransformer('all-mpnet-base-v2') # all-mpnet-base-v2 for quality performance\n",
    "cleaned_articles_for_embedding = [articles_ner_info[id_]['cleaned_article_text'] for id_ in article_ids] # Use cleaned noun text for embedding\n",
    "embeddings = model.encode(cleaned_articles_for_embedding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. Calculate Similarity Matrix (MOVED HERE)\n",
    "similarity_matrix = cosine_similarity(embeddings) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_ner_similarity(article_id1, article_id2, embedding_similarity):\n",
    "    ner_info1 = articles_ner_info[article_id1]\n",
    "    ner_info2 = articles_ner_info[article_id2]\n",
    "\n",
    "    person_entity_overlap = len(ner_info1[\"person_entities\"].intersection(ner_info2[\"person_entities\"]))\n",
    "\n",
    "    person_entity_diff_penalty = 0\n",
    "    if ner_info1[\"person_entities\"] and ner_info2[\"person_entities\"]:\n",
    "        person_entity_diff_ratio = 1 - (person_entity_overlap / min(len(ner_info1[\"person_entities\"]), len(ner_info2[\"person_entities\"])))\n",
    "        person_entity_diff_penalty = person_entity_diff_ratio * 0.93\n",
    "\n",
    "    refined_similarity = embedding_similarity - person_entity_diff_penalty  # Removed relationship keyword bonus\n",
    "\n",
    "    return max(0, refined_similarity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 7. Graph-Based Clustering with NER-aware Similarity\n",
    "threshold = 0.509  # Adjust threshold - might need to be lower as we are refining similarity\n",
    "graph = nx.Graph()\n",
    "\n",
    "for i in range(len(article_ids)):\n",
    "    id1 = article_ids[i]\n",
    "    graph.add_node(id1)\n",
    "    for j in range(i + 1, len(article_ids)):\n",
    "        id2 = article_ids[j]\n",
    "        embedding_similarity = similarity_matrix[i, j] # Get embedding similarity\n",
    "        refined_similarity = calculate_ner_similarity(id1, id2, embedding_similarity) # Calculate NER-aware similarity\n",
    "\n",
    "        if refined_similarity > threshold:\n",
    "            graph.add_edge(id1, id2, weight=refined_similarity) # You can store similarity as edge weight\n",
    "\n",
    "connected_components = list(nx.connected_components(graph))\n",
    "\n",
    "unique_ids = {}\n",
    "next_unique_id = 1\n",
    "\n",
    "for component in connected_components:\n",
    "    for article_id in component:\n",
    "        unique_ids[article_id] = next_unique_id\n",
    "    next_unique_id += 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 8. Add \"unique_article_ID\" column to DataFrame\n",
    "df['unique_article_ID'] = df['ID'].map(unique_ids) # efficient way to add the column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 9. Save the updated TSV file\n",
    "output_tsv_file = \"C:\\\\Users\\\\WINDOWS11\\\\Desktop\\\\kpop_agenda\\\\Step1\\\\articles_metadata_deduplicated.tsv\" # Change the output path\n",
    "df.to_csv(output_tsv_file, sep='\\t', encoding='utf-8', index=False)\n",
    "\n",
    "print(f\"Deduplicated TSV file saved to: {output_tsv_file}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
